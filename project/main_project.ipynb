{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "eca7d96c",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "---\n",
                "- Machine Learning applications in the field of Medicine to support and help the diagnosis of various diseases is crucial, to catch these at an early state. For this the algorithms\n",
                "used have to take into account the most telling characteristics about the tests performed to analyze the targeted anatomical part of the patient. This is true in the particular case of Brain Tumors where there are typical exams that are prescribged in order to find if there is a tumor in the cerebral cortex of the patient.\n",
                "- With the intent to build a model that classifies the imagiological results of tests like CT scans and MRIs into 'healty' or 'cancer' classes, to support the specialists decisions, we decided to train said model with a dataset with about 4600 unique samples of these types of exams, consisting their image results. \n",
                "-Given time we will explore the classification between different types of cancer in the brain cancer category.\n",
                "---\n",
                "## Step 1 - Data Exploration and Preprocessing\n",
                "\n",
                "- Inspecting the dataset structure and labels:  \n",
                "  - here is where we can separate the dataset through the labels Cancer/Healthy;\n",
                "  - given the separation we may start to operate in the dataset\n",
                "\n",
                "- The 1st operation needed is to reduce the images to a fixed shape, normalizing them into the same resolution;\n",
                "- Next we also need to split the dataset as said in the project proposal, in \"Training Data\" and \"Testing Data\";\n",
                "---\n",
                "#### Process\n",
                "- For this step then we will use \"pandas\" library to read the CSV metadata in order to get the information provided about the images\n",
                "- When this is completed we will separate the data into \"Healthy\" and \"Cancer\" lists\n",
                "- After this the main preprocessing of the data will begin "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "51cd8b89",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\\\n",
                "DEBUG = True\n",
                "\n",
                "csv_file = \"./data/metadata_rgb_only.csv\"\n",
                "\n",
                "\"\"\"This method reads the data from a CSV file\n",
                "returning said data.\"\"\"\n",
                "def read_data(csv_file):\n",
                "    df = pd.read_csv(csv_file)\n",
                "    return df \n",
                "\n",
                "data = read_data(csv_file)\n",
                "\n",
                "if DEBUG == True:\n",
                "    print(f\"ðŸ”´--------------ðŸ”´   Debug   ðŸ”´--------------ðŸ”´\\n\")\n",
                "    print(data)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f5cc767f",
            "metadata": {},
            "source": [
                "#### Step 1 - Completed\n",
                "\n",
                "- For now we have a variable `data` that has all the information in the file metadata stored in an N column matrix. Since this is the case we can pick and choose the data that we need to separate in the different classes. \n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2686f227",
            "metadata": {},
            "source": [
                "## Step 2 - Splitting dataset by its label\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ef6d192c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_list(data, column):\n",
                "    list = data[column] # get the info on column class\n",
                "    return list \n",
                "\n",
                "# Get the image labels\n",
                "labels = get_list(data, column=\"class\")\n",
                "\n",
                "if DEBUG == True:\n",
                "    print(f\"ðŸ”´--------------ðŸ”´   Debug   ðŸ”´--------------ðŸ”´\\n\")\n",
                "    print(labels, sep=\"\\n\")\n",
                "\n",
                "# Get the images list\n",
                "images = get_list(data, column=\"image\")\n",
                "\n",
                "if DEBUG == True:\n",
                "    print(f\"ðŸ”´--------------ðŸ”´   Debug   ðŸ”´--------------ðŸ”´\\n\")\n",
                "    print(images)\n",
                "\n",
                "# Split the images into healty/cancer\n",
                "cancer  = images[labels==\"tumor\"]\n",
                "healthy = images[labels==\"normal\"]\n",
                "\n",
                "print(cancer, healthy, sep=\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8bce600",
            "metadata": {},
            "source": [
                "### Step 2 - Completed\n",
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71692ec2",
            "metadata": {},
            "source": [
                "## Step 3 - Creation of Train and Testing datasets\n",
                "---\n",
                "- This allows to separate already both datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7842c07a",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "import random as rand\n",
                "\n",
                "DATA_DIR = \"./data\"    \n",
                "TRAIN_DIR = \"./data/train\"\n",
                "TEST_DIR  = \"./data/test\"\n",
                "TEST_SPLIT = 0.2\n",
                "SEED = 42\n",
                "\n",
                "rand.seed(SEED)\n",
                "\n",
                "classes = [\"cancer\", \"healthy\"]\n",
                "\n",
                "for cls in classes:\n",
                "    os.makedirs(os.path.join(TRAIN_DIR, cls), exist_ok=True)\n",
                "    os.makedirs(os.path.join(TEST_DIR, cls), exist_ok=True)\n",
                "\n",
                "    files = [f for f in os.listdir(os.path.join(DATA_DIR, cls)) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
                "    rand.shuffle(files)\n",
                "\n",
                "    split_idx = int(len(files) * TEST_SPLIT)\n",
                "    for f in files[:split_idx]:\n",
                "        shutil.copy2(os.path.join(DATA_DIR, cls, f), os.path.join(TEST_DIR, cls, f))\n",
                "    for f in files[split_idx:]:\n",
                "        shutil.copy2(os.path.join(DATA_DIR, cls, f), os.path.join(TRAIN_DIR, cls, f))\n",
                "\n",
                "print(\"âœ… Dataset split done!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "05cabf4a",
            "metadata": {},
            "source": [
                "### Step 3 - Completed\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9eb078ad",
            "metadata": {},
            "source": [
                "## Step 4 - Plotting images\n",
                "---\n",
                "- Separation of the images into the binary classes predetermined: `cancer` & `healthy` \n",
                "- With this we have the possibility of determining the priors for example\n",
                "- We should also split the dataset in `training` and `testing data` @ this point"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "143c2bbb",
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "\n",
                "\n",
                "cancer_img_dir= \"./data/cancer\"\n",
                "healthy_img_dir = \"./data/healthy\"\n",
                "\n",
                "N = 5\n",
                "\n",
                "def random_plot(image_list, label, image_dir, n=N):\n",
                "    plt.figure(figsize=(12,2))\n",
                "    for i, img_name in enumerate(image_list[:n]):\n",
                "        img_path = os.path.join(image_dir, img_name)\n",
                "        img = Image.open(img_path)\n",
                "        plt.subplot(1, n, i+1)\n",
                "        plt.imshow(img)\n",
                "        plt.axis(\"off\")\n",
                "        plt.title(label)\n",
                "    plt.show()\n",
                "\n",
                "if DEBUG == True:\n",
                "    print(\"ðŸ”´--------------ðŸ”´   Debug   ðŸ”´--------------ðŸ”´\")\n",
                "    random_plot(cancer.to_list(), \"tumor\", cancer_img_dir)\n",
                "    random_plot(healthy.to_list(), \"normal\", healthy_img_dir)\n",
                "    print(f\"Number of cancer images: {len(cancer)}\")\n",
                "    print(f\"Number of healthy images: {len(healthy)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2602de57",
            "metadata": {},
            "source": [
                "### Step 4 - Completed\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b00dd646",
            "metadata": {},
            "source": [
                "## Step 5 - Image Preprocessing/Normalization/Reshape and Splitting\n",
                "---\n",
                "- Since the images have different resolutions and formats they should be normalized to the same size in order to build a solid foundation of comparison to train the model.\n",
                "- To deal with that processing we decided to resize all images to a 256x256 resolution, although that number can be changed.\n",
                "    - All 'L' format images shall be converted to 'RGB'.\n",
                "\n",
                "- These will be split in an 80/20 where both classes have to be reasonably represented in the samples, especially in the training data\n",
                "- Explain datagen.flow and respective variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69b09cdc",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    validation_split=0.2   # 20% Testing 80% Training\n",
                ")\n",
                "\n",
                "batch_size = 4\n",
                "img_size = (128,128)\n",
                "\n",
                "train_gen = datagen.flow_from_directory( # flow_from_directory() by default converts to RGB\n",
                "    TRAIN_DIR,\n",
                "    target_size=img_size,\n",
                "    batch_size=batch_size,\n",
                "    class_mode='binary',\n",
                "    subset='training',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "val_gen = datagen.flow_from_directory(\n",
                "    TRAIN_DIR,\n",
                "    target_size=img_size,\n",
                "    batch_size=batch_size,\n",
                "    class_mode='binary',\n",
                "    subset='validation',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "if DEBUG == True:\n",
                "    print(\"ðŸ”´--------------ðŸ”´   Debug   ðŸ”´--------------ðŸ”´\")\n",
                "    images, labels = next(train_gen)  # batch_size images\n",
                "\n",
                "    # Print shape of the first image to confirm size\n",
                "    print(\"Shape of first image:\", images[0].shape)  # should be (128,128,3) if you set target_size=(128,128)\n",
                "\n",
                "    # Plot first N images in the batch\n",
                "    N = min(8, len(images))  # just in case batch_size < 8\n",
                "    plt.figure(figsize=(16, 4))\n",
                "    for i in range(N):\n",
                "        plt.subplot(1, N, i+1)\n",
                "        plt.imshow(images[i])\n",
                "        plt.axis('off')\n",
                "        plt.title('cancer' if labels[i]==0 else 'healthy')\n",
                "    plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "585d2764",
            "metadata": {},
            "source": [
                "### Step 5 - Completed\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51ae6654",
            "metadata": {},
            "source": [
                "## Step 6 - Validating CNN model \n",
                "---\n",
                "- As the chosen model totally reflects the results of our program, we decided to stick to CNN (Convolutional Neural Network) that is pretty reliable when it comes to analyzing image, video and music data.\n",
                "- Source of inspiration : https://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial\n",
                "- An important step when creating the CNN model is to make able to be trained to the future feature extracting process"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f02f6c4f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from tensorflow.keras.models import Sequential\n",
                "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
                "\n",
                "# model = Sequential([\n",
                "#     Conv2D(16, (3,3), activation='relu', input_shape=(128,128,3)),\n",
                "#     MaxPooling2D(2,2),\n",
                "#     Conv2D(32, (3,3), activation='relu'),\n",
                "#     MaxPooling2D(2,2),\n",
                "#     Conv2D(64, (3,3), activation='relu'),\n",
                "#     MaxPooling2D(2,2),\n",
                "#     Flatten(),\n",
                "#     Dense(32, activation='relu'),\n",
                "#     Dropout(0.5),\n",
                "#     Dense(1, activation='sigmoid')\n",
                "# ])\n",
                "\n",
                "# model.compile(\n",
                "#     optimizer='adam',\n",
                "#     loss='binary_crossentropy',\n",
                "#     metrics=['accuracy', 'precision', 'recall']\n",
                "# )\n",
                "\n",
                "# history = model.fit(\n",
                "#     train_gen,        \n",
                "#     validation_data=val_gen,  # To confront to validate results\n",
                "#     epochs=30,\n",
                "#     verbose=1\n",
                "# )\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.applications import MobileNetV2\n",
                "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
                "from tensorflow.keras.models import Model\n",
                "\n",
                "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
                "base_model.trainable = True \n",
                "\n",
                "# unfreezing the last 40 layers for fine-tuning\n",
                "for layer in base_model.layers[:-40]:\n",
                "    layer.trainable = False\n",
                "\n",
                "# classifier head\n",
                "x = base_model.output\n",
                "x = GlobalAveragePooling2D()(x)\n",
                "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
                "x = tf.keras.layers.Dropout(0.4)(x)\n",
                "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
                "\n",
                "model = Model(inputs=base_model.input, outputs=outputs)\n",
                "\n",
                "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'precision', 'recall'])\n",
                "\n",
                "history = model.fit(\n",
                "    train_gen,\n",
                "    validation_data=val_gen,\n",
                "    epochs=12,\n",
                "    steps_per_epoch=len(train_gen),\n",
                "    validation_steps=len(val_gen),\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "# this part implements an optional fine-tuning of the base model - did not test this yet\n",
                "# base_model.trainable = True\n",
                "# for layer in base_model.layers[:-25]:  # freeze earlier layers\n",
                "#     layer.trainable = False\n",
                "\n",
                "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # lower LR for fine-tuning\n",
                "#               loss='binary_crossentropy',\n",
                "#               metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
                "\n",
                "# history_fine = model.fit(\n",
                "#     train_gen,\n",
                "#     validation_data=val_gen,\n",
                "#     epochs=6,\n",
                "#     steps_per_epoch=len(train_gen),\n",
                "#     validation_steps=len(val_gen),\n",
                "#     verbose=1\n",
                "# )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6fb3860",
            "metadata": {},
            "source": [
                "#### Step 6 - Completed\n",
                "\n",
                "-  Using `MobileNetV2` pre-trained model set to not trainable we add a classifier head with the `GlobalAveragePooling2D`which produces and output which corresponds to a single signamoid neuron for binary classification.\n",
                "-  then we compile the model with the `adam` optimization, loss as `binary_crossentropy` and the assessment metrics are : `accuracy`, `precision`, `recall` \n",
                "- after we use `train_gen` and `val_gen` to train the model splitting the steps by using epochs and limiting the number of epoch steps to the values `steps_per_epoch` and `validation_steps`\n",
                "- Essentialy corresponds to training a base model\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "398b6792",
            "metadata": {},
            "source": [
                "## Step 7 - Evaluating the trained Base Model on the test data\n",
                "\n",
                "- here we are taking a look at the metrics of the model trained when it omes to the test data\n",
                "- we also want to build the confusion matrix to figure out what the false negatives and the false positives look like in the current model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f845528",
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score, \n",
                "                             precision_recall_curve, average_precision_score, precision_score,\n",
                "                             recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef)\n",
                "\n",
                "test_gen= datagen.flow_from_directory(\n",
                "  TEST_DIR,\n",
                "  target_size=img_size,\n",
                "  batch_size=batch_size,\n",
                "  class_mode='binary',\n",
                "  shuffle=False\n",
                ")\n",
                "\n",
                "steps=math.ceil(test_gen.samples / batch_size)\n",
                "\n",
                "# Prediction (probabilities)\n",
                "y_prob=model.predict(test_gen, steps=steps, verbose=1).ravel()\n",
                "y_true=test_gen.classes\n",
                "y_pred=(y_prob>=0.5).astype(int)\n",
                "\n",
                "# Metrics - Important for the model assessment -accuracy was said (in class) to not be the best for cancer applications - altough i think these 10 should be enough ;)\n",
                "cm=confusion_matrix(y_true, y_pred)\n",
                "tn, fp, fn, tp= cm.ravel()\n",
                "sensitivity= tp / (tp + fn) if(tp + fn) > 0  else 0.0\n",
                "specificity= tn / (tn + fp) if(tn+fp) > 0 else 0.0\n",
                "precision=precision_score(y_true, y_pred, zero_division=0)\n",
                "recall=recall_score(y_true, y_pred, zero_division=0)\n",
                "f1=f1_score(y_true, y_pred, zero_division=0)\n",
                "roc_auc=roc_auc_score(y_true, y_prob)\n",
                "ap=average_precision_score(y_true, y_prob)\n",
                "mcc=matthews_corrcoef(y_true, y_pred)\n",
                "ba=balanced_accuracy_score(y_true, y_pred)\n",
                "\n",
                "print(\"Class mapping (from generator):\", test_gen.class_indices)\n",
                "print(\"\\nConfusion Matrix: \\n\", cm)\n",
                "print(f\"\\nSensitivity (Recall): {sensitivity:.4f}\")\n",
                "print(f\"Specificity: {specificity:.4f}\")\n",
                "print(f\"precision: {precision:.4f}\")\n",
                "print(f\"recall: {recall:.4f}\")\n",
                "print(f\"F1 score: {f1:.4f}\")\n",
                "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
                "print(f\"Average Precision (AUPRC): {ap:.4f}\")\n",
                "print(f\"MCC: {mcc:.4f}\")\n",
                "print(f\"Balanced Accuracy: {ba:.4f}\")\n",
                "\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=[k for k,v in sorted(test_gen.class_indices.items(), key=lambda x:x[1])]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc78bf2c",
            "metadata": {},
            "source": [
                "#### Step 7 - Completed\n",
                "\n",
                "- Looking at the data there is in the confuction matrix interesting info. There were 475 classification of images into the class cancer which were in fact cancer and 407 classifications as healthy which were in fact healthy. \n",
                "- however there were some false positves and false negatives - we should now work on a way to reduce the number of the false negatives\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2f93e44e",
            "metadata": {},
            "source": [
                "## Step 8 - Extracting features to use in SVM model\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecd7edde",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "feature_output = GlobalAveragePooling2D()(base_model.output)\n",
                "feature_extractor = Model(inputs=base_model.input, outputs=feature_output)\n",
                "\n",
                "def extract_features(generator, extractor_model):\n",
                "    steps = math.ceil(generator.samples / generator.batch_size)\n",
                "    feats = extractor_model.predict(generator, steps=steps, verbose=1)\n",
                "    labels = generator.classes  \n",
                "    return feats, labels\n",
                "\n",
                "X_train_feats, y_train = extract_features(train_gen, feature_extractor)\n",
                "X_val_feats,   y_val   = extract_features(val_gen, feature_extractor)\n",
                "X_test_feats,  y_test  = extract_features(test_gen, feature_extractor)\n",
                "\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train_feats)\n",
                "X_val_scaled   = scaler.transform(X_val_feats)\n",
                "X_test_scaled  = scaler.transform(X_test_feats)\n",
                "\n",
                "# np.save('X_train_feats.npy', X_train_feats); np.save('y_train.npy', y_train)\n",
                "model.summary()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b84fa105",
            "metadata": {},
            "source": [
                "## Step 9 - Training and Evaluating SVM\n",
                "- Here we put the SVM to the test, applying a different set of characteristics involving the features extracted previously.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca95094e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.svm import SVC\n",
                "\n",
                "svm = SVC(kernel='rbf', C=10, probability=True, class_weight='balanced')  # try class_weight='balanced' if classes are imbalanced\n",
                "svm.fit(X_train_scaled, y_train)\n",
                "\n",
                "y_prob = svm.predict_proba(X_test_scaled)[:,1]   # probability for positive class\n",
                "y_pred = (y_prob >= 0.5).astype(int)\n",
                "\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "tn, fp, fn, tp = cm.ravel() if cm.size==4 else (0,0,0,0)\n",
                "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
                "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
                "precision = precision_score(y_test, y_pred, zero_division=0)\n",
                "recall = recall_score(y_test, y_pred, zero_division=0)\n",
                "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
                "roc_auc = roc_auc_score(y_test, y_prob)\n",
                "ap = average_precision_score(y_test, y_prob)\n",
                "mcc = matthews_corrcoef(y_test, y_pred)\n",
                "ba = balanced_accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(\"Class mapping (from generator):\", test_gen.class_indices)\n",
                "print(\"\\nConfusion Matrix: \\n\", cm)\n",
                "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
                "print(f\"Specificity: {specificity:.4f}\")\n",
                "print(f\"precision: {precision:.4f}\")\n",
                "print(f\"recall: {recall:.4f}\")\n",
                "print(f\"F1 score: {f1:.4f}\")\n",
                "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
                "print(f\"Average Precision (AUPRC): {ap:.4f}\")\n",
                "print(f\"MCC: {mcc:.4f}\")\n",
                "print(f\"Balanced Accuracy: {ba:.4f}\")\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=[k for k,v in sorted(test_gen.class_indices.items(), key=lambda x:x[1])]))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e8dcb012",
            "metadata": {},
            "source": [
                "## Step 10 - Monte Carlo method\n",
                "- Applied to obtain a 95% degree confidence interval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b44b31c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "n_boot = 1000  # number of bootstrap samples\n",
                "rng = np.random.default_rng(seed=42)  # reproducibility\n",
                "\n",
                "f1_scores = []\n",
                "roc_scores = []\n",
                "precision_scores = []\n",
                "recall_scores = []\n",
                "ba_scores = []\n",
                "mcc_scores = []\n",
                "\n",
                "for _ in range(n_boot):\n",
                "    indices = rng.choice(len(y_test), len(y_test), replace=True)\n",
                "    y_true_bs = y_test[indices]\n",
                "    y_prob_bs = y_prob[indices]\n",
                "    y_pred_bs = (y_prob_bs >= 0.5).astype(int)\n",
                "    \n",
                "    f1_scores.append(f1_score(y_true_bs, y_pred_bs, zero_division=0))\n",
                "    roc_scores.append(roc_auc_score(y_true_bs, y_prob_bs))\n",
                "    precision_scores.append(precision_score(y_true_bs, y_pred_bs, zero_division=0))\n",
                "    recall_scores.append(recall_score(y_true_bs, y_pred_bs, zero_division=0))\n",
                "    ba_scores.append(balanced_accuracy_score(y_true_bs, y_pred_bs))\n",
                "    mcc_scores.append(matthews_corrcoef(y_true_bs, y_pred_bs))\n",
                "\n",
                "def ci(arr):\n",
                "    return np.percentile(arr, [2.5, 97.5])\n",
                "\n",
                "print(\"\\nBootstrap 95% Confidence Intervals:\")\n",
                "print(f\"F1 score: {np.mean(f1_scores):.4f}, 95% CI: {ci(f1_scores)}\")\n",
                "print(f\"ROC AUC: {np.mean(roc_scores):.4f}, 95% CI: {ci(roc_scores)}\")\n",
                "print(f\"Precision: {np.mean(precision_scores):.4f}, 95% CI: {ci(precision_scores)}\")\n",
                "print(f\"Recall: {np.mean(recall_scores):.4f}, 95% CI: {ci(recall_scores)}\")\n",
                "print(f\"Balanced Accuracy: {np.mean(ba_scores):.4f}, 95% CI: {ci(ba_scores)}\")\n",
                "print(f\"MCC: {np.mean(mcc_scores):.4f}, 95% CI: {ci(mcc_scores)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
